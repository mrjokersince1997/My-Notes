# Kafka

---

## 消息系统

### 基本概念

使用专用的中间件在网络中传输数据。通常由专用的机器集群来实现，从发送方接收数据，再将数据转发给相应的接收方。

使用消息系统具有以下优势：

1. 将发送方和接收方解耦，统一使用消息系统提供的接口进行通信，易修改易扩展。
2. 能够持久化保存数据，防止处理数据失败导致数据丢失。
3. 分布式系统能够均衡负载，且能根据负载灵活调整机器数量，能够处理高吞吐量和流量突增的情况。

除此之外，消息系统还可以保障：

4. 一部分组件失效时，不会影响到整个系统。
5. 数据处理的顺序不被打乱。
6. 通过缓冲层控制和优化数据流经过系统的速度。
7. 提供了异步处理机制，允许用户把消息放入队列， 但并不立即处理它。

---

## kafka 

### 基本介绍

使用 scala 开发，是当前最常用的消息系统之一。目前已捐献给 Apache 基金会。一般应用于大数据日志处理。

- **优势**

使用分片 partition 把一个队列的流量均匀分散在多台机器上，IO 吞吐量非常高。

- **劣势**

不进行消息重复性检查，可能导致消费重复数据或者异常情况下的数据丢失。实时性方面也存在少量延迟。


### 整体架构

![kafka](kafka.jpeg)

- `Cluster`

负责处理 Message 的集群，内部包含多个服务器。作为一个整体去处理特定的任务。

- `Broker`

Cluster 内的单个服务器，负责接收外部请求，并把 Message 持久化到本地磁盘。

每个 Cluster 当中会选举出一个 Broker 来担任 Controller，负责处理 Partition 的 Leader 选举，协调 Partition 迁移等工作。

- `Topic`

逻辑上对 Message 分类，一个 Topic 可以分布在多个 Broker 上。

- `Partition`

物理上对 Topic 划分，每个 Partition 完整存放在一个 Broker 上。

- `Producer`

消息发布者，向 Kafka broker 发布消息。

- `Consumer`

消息消费者，向 Kafka broker 读取消息。

### 消息定位

- `Offset`

表示 Message 在 Partition 中的偏移量，唯一确定了 Partition 中的一条 Message 。

- `Consumer Lag`

消息发送和消费位置的差值。每个 consumer group 相互独立，反映目前该 Consumer Group 整体消费滞后情况。

- `Log Segment`

partition 的内部分段，包括了一个数据文件和一个索引文件。使得可以在一个较小的数据文件中查找对应offset的Message。

### 数据分片模型

![group](group.jpeg)

- `Consumer Group`

每个 Consumer 属于一个特定的 Consumer Group。

对于订阅的消息，每个 Consumer Group 都能消费全量的消息，但其中每个 Consumer 只能消费其中一部分 Partition 。

Partition 数量推荐设为 Consumer 数量的整数倍，便于均分。


### 多副本模型


为了支持高可用，Kafka支持对一个topic的partition提供多副本，同一个partition的不同副本会存放在不同的broker上，这样当同时不可用的kafka broker数量小于副本的数目时，对应的partition仍然可以被读取数据。


- `Replica`

Partition 的副本，来保障 Partition 的高可用，但同时会降低整体吞吐。

默认为 3 个副本：1 个 leader 副本，2 个 follower 副本。每个 Replication 集合中的 Partition 都会选出一个唯一的 Leader，所有的读写请求都由 Leader 处理。其他 Replicas 从 Leader 处把数据更新同步到本地。

- `ISR`

Leader 当前能够 Catch-up 的 Replicas 集合，每个 Leader Partition 都有自己独立的 ISR 。如果 Replica 数据同步的延迟过高(延迟时间或者延迟条数超过阈值)，该 Replica 会被暂时踢出 ISR。

---

## Java 操作 Kafka

### 导入依赖

pom.xml

```xml
<!-- 导入 0.10.2 版本 kafka -->
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>0.10.2.0</version>
</dependency>
```

### 生产者

```java
import java.util.Properties;
import java.util.Random;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;

public class Producer {
    public static String topic = "test";                                                            

    public static void main(String[] args) throws InterruptedException {
        // 配置 kafka
        Properties p = new Properties();
        p.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "192.168.23.76:9092,192.168.23.77:9092");          // kafka 地址，多个地址用逗号分割（必备）
        p.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);                        // key 的序列化类（必备）
        p.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);                      // value 的序列化类（必备）
        p.put("request.required.acks", "-1");                                                             // 消息系统向发送者返回 ACK： 0 马上 / 1 主服务器收到 / -1 所有服务器收到后                           
        p.put("producer.type", "async");                                                                  // 异步生产，批量存入缓存后再发到服务器

        // 填充配置,初始化生产者
        KafkaProducer<String, String> kafkaProducer = new KafkaProducer<>(p);

        // 向指定主题发送消息
        try {
            while (true) {
                String msg = "Hello," + new Random().nextInt(100);
                ProducerRecord<String, String> record = new ProducerRecord<String, String>(topic, msg);   // 指定消息和要发送到的主题
                kafkaProducer.send(record);                                                               // 发送消息，可以通过返回的 Future 判断是否已经发送到 kafka
                // kafkaProducer.send(record);                                                               使用 send 的第二个参数来回调，通过回调判断是否发送成功
                System.out.println("消息发送成功:" + msg);
                Thread.sleep(500);
            }
        } finally {
            kafkaProducer.close();
        }

    }
}
```


### 消费者

```java
import java.util.Collections;
import java.util.Properties;

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.StringDeserializer;

public class Consumer {
    public static void main(String[] args) {
        // 配置 kafka
        Properties p = new Properties();
        p.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "192.168.23.76:9092");                            // zookeeper 地址
        p.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);                   // key 的反序列化类
        p.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);                     // value 的序列化类
        p.put(ConsumerConfig.GROUP_ID_CONFIG, "duanjt_test");                                            // 消费者分组名                                

        // 填充配置,初始化消费者
        KafkaConsumer<String, String> kafkaConsumer = new KafkaConsumer<String, String>(p);
        // 向指定主题订阅消息
        kafkaConsumer.subscribe(Collections.singletonList(Producer.topic));

        while (true) {
            // 从订阅接收消息
            ConsumerRecords<String, String> records = kafkaConsumer.poll(100);
            for (ConsumerRecord<String, String> record : records) {
                System.out.println(String.format("topic:%s,offset:%d,消息:%s", //
                        record.topic(), record.offset(), record.value()));
            }
        }
    }
}
```



